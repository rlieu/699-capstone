{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlieu/699-capstone/.venv/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "[nltk_data] Downloading package stopwords to /Users/rlieu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rlieu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rlieu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rlieu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x350bef040>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "# import itertools\n",
    "import re\n",
    "# import string\n",
    "import warnings\n",
    "\n",
    "# Third-party library imports for general purposes\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import scipy.sparse as sp\n",
    "# import statsmodels.api as sm\n",
    "# from scipy.sparse import load_npz\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, mean_absolute_error, mean_squared_error, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.preprocessing import MaxAbsScaler, StandardScaler, scale\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.decomposition import PCA, NMF\n",
    "# from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "# from tpot.builtins import StackingEstimator\n",
    "# from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# import plotly.express as px\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Text processing and NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# Gensim for topic modeling and vector space modeling\n",
    "# from gensim import corpora, models\n",
    "# from gensim.models.coherencemodel import CoherenceModel\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Spacy for advanced NLP\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Statsmodels for statistical modeling\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "# Auto ARIMA model\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# Plotting and visualization\n",
    "# from pandas.plotting import parallel_coordinates\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Miscellaneous\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# NLTK downloads for specific functionalities\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import plotly.figure_factory as ff\n",
    "# from sklearn.cluster import KMeans\n",
    "from IPython.display import Image\n",
    "import plotly.io as pio\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['careerguidance', 'resumes', 'ITCareerQuestions',\n",
       "       'FinancialCareers', 'LegalAdviceOffTopic', 'teachers', 'AskHR',\n",
       "       'sales', 'jobs', 'cscareerquestions'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ready_data_score.csv')\n",
    "\n",
    "df.shape\n",
    "#Removed target variable due to insufficent count \n",
    "df = df[df['subreddit']!='EngineeringCareers']\n",
    "df['subreddit'] = df['subreddit'].replace({'careeradvice': 'careerguidance'})\n",
    "\n",
    "# show remaining target variables and count\n",
    "# df['subreddit'].value_counts()\n",
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature engineering we will need to convert desired features in to numerical representation \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'subreddit' column to obtain encoded target variable\n",
    "df['subreddit_encoded'] = label_encoder.fit_transform(df['subreddit'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to weekday and weekend One-hot encode 'day_of_week' and 'month', obtaining a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_id</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>subreddit_encoded</th>\n",
       "      <th>day_of_week_encoded</th>\n",
       "      <th>month_encoded</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi context year old guy Amsterdam currently em...</td>\n",
       "      <td>Lazy job or Hard job?</td>\n",
       "      <td>Weak_Assumption_6889</td>\n",
       "      <td>8</td>\n",
       "      <td>1bfpxll</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/careeradvice/comments...</td>\n",
       "      <td>careerguidance</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>March</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Looking new role havenut much traction Recentl...</td>\n",
       "      <td>Roast my Resume Pls</td>\n",
       "      <td>Neither_Trash</td>\n",
       "      <td>1</td>\n",
       "      <td>1bh8md2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/n918fjprlyoc1.jpeg</td>\n",
       "      <td>resumes</td>\n",
       "      <td>Review my resume â€¢ I'm in North America</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>March</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I M west bengal bachelor Arts Self taught deve...</td>\n",
       "      <td>Is Jadavpur University good for MCA?</td>\n",
       "      <td>grvx_rdt</td>\n",
       "      <td>0</td>\n",
       "      <td>1bfg926</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/careeradvice/comments...</td>\n",
       "      <td>careerguidance</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>March</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its created MS word I yoe Java spring bootnn</td>\n",
       "      <td>Review my resume please</td>\n",
       "      <td>GroundbreakingZone94</td>\n",
       "      <td>7</td>\n",
       "      <td>1bh3jpg</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/tjnti8vplxoc1.jpeg</td>\n",
       "      <td>resumes</td>\n",
       "      <td>Review my resume â€¢ I'm in Asia</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>March</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I live Los Angeles year oldn</td>\n",
       "      <td>Thoughts on My Experience?</td>\n",
       "      <td>AshamedJellyfish9197</td>\n",
       "      <td>1</td>\n",
       "      <td>1bgejk2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>https://i.redd.it/ifqv15ry3roc1.jpeg</td>\n",
       "      <td>resumes</td>\n",
       "      <td>Review my resume â€¢ I'm in North America</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>March</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Hi context year old guy Amsterdam currently em...   \n",
       "1  Looking new role havenut much traction Recentl...   \n",
       "2  I M west bengal bachelor Arts Self taught deve...   \n",
       "3       Its created MS word I yoe Java spring bootnn   \n",
       "4                       I live Los Angeles year oldn   \n",
       "\n",
       "                                  title                author  num_comments  \\\n",
       "0                 Lazy job or Hard job?  Weak_Assumption_6889             8   \n",
       "1                   Roast my Resume Pls         Neither_Trash             1   \n",
       "2  Is Jadavpur University good for MCA?              grvx_rdt             0   \n",
       "3               Review my resume please  GroundbreakingZone94             7   \n",
       "4            Thoughts on My Experience?  AshamedJellyfish9197             1   \n",
       "\n",
       "   post_id  upvote_ratio  score  \\\n",
       "0  1bfpxll          0.33      0   \n",
       "1  1bh8md2          0.99      1   \n",
       "2  1bfg926          0.66      1   \n",
       "3  1bh3jpg          0.60      1   \n",
       "4  1bgejk2          0.33      0   \n",
       "\n",
       "                                                 url       subreddit  \\\n",
       "0  https://www.reddit.com/r/careeradvice/comments...  careerguidance   \n",
       "1               https://i.redd.it/n918fjprlyoc1.jpeg         resumes   \n",
       "2  https://www.reddit.com/r/careeradvice/comments...  careerguidance   \n",
       "3               https://i.redd.it/tjnti8vplxoc1.jpeg         resumes   \n",
       "4               https://i.redd.it/ifqv15ry3roc1.jpeg         resumes   \n",
       "\n",
       "                           link_flair_text  ... hour_of_day  month  year  \\\n",
       "0                                  Unknown  ...          22  March  2024   \n",
       "1  Review my resume â€¢ I'm in North America  ...          21  March  2024   \n",
       "2                                  Unknown  ...          15  March  2024   \n",
       "3           Review my resume â€¢ I'm in Asia  ...          17  March  2024   \n",
       "4  Review my resume â€¢ I'm in North America  ...          19  March  2024   \n",
       "\n",
       "   sentiment_score subreddit_encoded  day_of_week_encoded  month_encoded  \\\n",
       "0           0.7579                 4                    5              3   \n",
       "1           0.6369                 7                    7              3   \n",
       "2           0.8651                 4                    5              3   \n",
       "3           0.2500                 7                    7              3   \n",
       "4           0.0000                 7                    6              3   \n",
       "\n",
       "   is_weekday  Unnamed: 0  topic  \n",
       "0           1           0    1.0  \n",
       "1           0           1    0.0  \n",
       "2           1           2    0.0  \n",
       "3           0           3    0.0  \n",
       "4           0           4    0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "day_mapping = {\n",
    "    \"Monday\": 1,\n",
    "    \"Tuesday\": 2,\n",
    "    \"Wednesday\": 3,\n",
    "    \"Thursday\": 4,\n",
    "    \"Friday\": 5,\n",
    "    \"Saturday\": 6,\n",
    "    \"Sunday\": 7\n",
    "}\n",
    "\n",
    "df['day_of_week_encoded'] = df['day_of_week'].map(day_mapping)\n",
    "\n",
    "month_mapping = {\n",
    "    \"January\": 1,\n",
    "    \"February\": 2,\n",
    "    \"March\": 3,\n",
    "    \"April\": 4,\n",
    "    \"May\": 5,\n",
    "    \"June\": 6,\n",
    "    \"July\": 7,\n",
    "    \"August\": 8,\n",
    "    \"September\": 9,\n",
    "    \"October\": 10,\n",
    "    \"November\": 11,\n",
    "    \"December\": 12\n",
    "}\n",
    "\n",
    "df['month_encoded'] = df['month'].map(month_mapping)\n",
    "\n",
    "\n",
    "df['is_weekday'] = df['day_of_week'].apply(lambda x: 1 if x in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] else 0)\n",
    "\n",
    "# topics\n",
    "df_topics = pd.read_csv(\"../Topic_Modeling/topic_labels.csv\")\n",
    "df = df.merge(df_topics, on=\"post_id\")\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "df['topic'] = enc.fit_transform(df['topic'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Create a Pandas DataFrame of the hot encoded column\n",
    "# ohe_df = pd.DataFrame(transformed, columns=[\"topic\"])\n",
    "# df = pd.concat([df, ohe_df], axis=1).drop(['topic'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#to provide more context concatenated text and title then preformed tfidf on the Concatenated title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # text = expand_contractions(text)\n",
    "    \n",
    "    # Removing unicode characters\n",
    "    text = re.sub(r'\\\\u[0-9A-Fa-f]+', '', text)\n",
    "    \n",
    "    # Removing escape sequences\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    text = re.sub(r'\\\\', '', text)\n",
    "    \n",
    "    # Removing non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stop words and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Joining back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "df['processed_title'] = df['title'].apply(preprocess_text)\n",
    "df['combined_text'] = df['processed_title'] + ' ' + df['processed_text']\n",
    "\n",
    "# Apply TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# Using matrices and want to combine with encoded_features, you will need to convert them to a dense format\n",
    "tfidf_dense = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the dense array with the array_features for one set of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inorder to cobine all features will have to convert them to an numpy array \n",
    "#this dense matrix gives a 7% higher accuracy score but is expensive computional wise \n",
    "#solely due to cost ran sparse matrix  but feel free to run either\n",
    "\n",
    "array_features = df[['topic','num_comments','upvote_ratio','score','hour_of_day','year','day_of_week_encoded','month_encoded','is_weekday','sentiment_score']].values\n",
    "\n",
    "combined_features = np.hstack((tfidf_dense, array_features))\n",
    "\n",
    "#target varibale\n",
    "y = df['subreddit_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n",
    "#Linear_SVC\n",
    "X_trains, X_tests, y_trains, y_tests = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hyper tuning techinques for both the linear svc and the XGB boost where a combination of and exhaustive grid search and Baysiean optimization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exhaustive grid XGB\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(1, 15, dtype=int)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'verbosity': hp.choice('verbosity', [0]),\n",
    "    'n_jobs': -1  # Use all available cores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [7:10:05<00:00, 258.05s/trial, best loss: -0.7367292225201073]   \n",
      "Best Hyperparameters for XGBoost: {'learning_rate': 0.04463961986243752, 'max_depth': 4, 'min_child_weight': 3.0, 'n_estimators': 3, 'subsample': 0.5501963118268534, 'verbosity': 0}\n"
     ]
    }
   ],
   "source": [
    "# Baysiean optimization XGB\n",
    "def objective(params):\n",
    "    xgb_classifier = XGBClassifier(**params)\n",
    "    score = cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "print(\"Best Hyperparameters for XGBoost:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(learning_rate=0.1132567200427148, max_depth=2, min_child_weight=3.0, n_estimators=100, n_jobs=1, subsample=0.6821295850917053, verbosity=0)\n",
    "XGB.fit(X_train,y_train)\n",
    "results = XGB.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 67% accuracy the dense matrix and pairing of features gave us a 6 percent increase in overall preformance then tthe linearSVC suggested with the Auto ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.729807005003574\n",
      "Precision: 0.7425486080497075\n",
      "Recall:0.729807005003574\n",
      "F1 Score: 0.727178598801836\n"
     ]
    }
   ],
   "source": [
    "#XGBClassifier measurement statistics \n",
    "acc = accuracy_score(y_test,results)\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "#precision\n",
    "precision = precision_score(y_test,results,average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "#Recall\n",
    "recall = recall_score(y_test, results, average='weighted')\n",
    "print(f'Recall:{recall}')\n",
    "#f1 Score \n",
    "f1 = f1_score(y_test,results,average='weighted')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of tpot is to assist with model selection tpot gave us a great bench mark model a Linear_SVC that gave good accuracy but after tuning and a pca analysis the final model choosen was an XGboost classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                     \n",
      "Generation 1 - Current best internal CV score: 0.6627345844504021\n",
      "                                                                                    \n",
      "Generation 2 - Current best internal CV score: 0.6627345844504021\n",
      "                                                                                   \n",
      "Generation 3 - Current best internal CV score: 0.6627345844504021\n",
      "                                                                                   \n",
      "Generation 4 - Current best internal CV score: 0.6627345844504021\n",
      "                                                                                   \n",
      "Generation 5 - Current best internal CV score: 0.6627345844504021\n",
      "                                                                                   \n",
      "Generation 6 - Current best internal CV score: 0.6629133154602324\n",
      "                                                                                   \n",
      "Generation 7 - Current best internal CV score: 0.6629133154602324\n",
      "                                                                                   \n",
      "Generation 8 - Current best internal CV score: 0.6629133154602324\n",
      "                                                                                   \n",
      "Generation 9 - Current best internal CV score: 0.6629133154602324\n",
      "                                                                                  \n",
      "Generation 10 - Current best internal CV score: 0.6629133154602324\n",
      "                                                              \n",
      "Best pipeline: LinearSVC(input_matrix, C=0.5, dual=False, loss=squared_hinge, penalty=l1, tol=0.0001)\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=10,population_size=100,verbosity=2,random_state=42, config_dict='TPOT sparse')\n",
    "tpot.fit(X_trains, y_trains)\n",
    "tpot.export('best_tpot_pipelineV2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = LinearSVC(C= 0.09695023762026912,dual=False,loss='squared_hinge',penalty='l2',tol=0.000143728759546748,class_weight='balanced') \n",
    "#fix random state in exported estimator\n",
    "if hasattr(exported_pipeline,'random_state'):\n",
    "    setattr(exported_pipeline,'random_state',42)\n",
    "    \n",
    "exported_pipeline.fit(X_trains,y_trains)\n",
    "result = exported_pipeline.predict(X_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "Best Score: 0.6812008577555396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#exhaustive grid LinearSVC\n",
    "param_grid = {'C':[.01,.1,1,10,100,1000],\n",
    "              'tol':[.0001,.001,.01,.1],\n",
    "              'loss':['squared_hinge'],\n",
    "              'penalty':['l2'],\n",
    "              'dual':[False]}\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "lin_svc =LinearSVC(random_state=42)\n",
    "\n",
    "best_score = 0\n",
    "best_params={}\n",
    "\n",
    "for params in tqdm(grid, desc=\"GridSearch\"):\n",
    "    \n",
    "    lin_svc.set_params(**params)\n",
    "    lin_svc.fit(X_trains,y_trains)\n",
    "    #Evaluate the model\n",
    "    current_score = lin_svc.score(X_tests,y_tests)\n",
    "    \n",
    "    #update with best score and param\n",
    "    \n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        best_params = params \n",
    "        \n",
    "print(\"Best Parameters:\",best_params)\n",
    "print(\"Best Score:\",best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:41<00:00,  2.40trial/s, best loss: -0.6661304736371761]\n",
      "Best Hyperparameters: {'C': 0.18144248431339347, 'dual': 0, 'loss': 0, 'penalty': 0, 'tol': 0.005054052637811301}\n"
     ]
    }
   ],
   "source": [
    "# Baysiean optimization LinearSVC\n",
    "space = {\n",
    "    'C': hp.loguniform('C', np.log(0.01), np.log(1000)),\n",
    "    'tol': hp.loguniform('tol', np.log(0.0001), np.log(0.1)),\n",
    "    'loss': hp.choice('loss', ['squared_hinge']),\n",
    "    'penalty': hp.choice('penalty', ['l2']),\n",
    "    'dual': hp.choice('dual', [False])\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    lin_svc = LinearSVC(**params, random_state=42)\n",
    "    score = cross_val_score(lin_svc, X_trains, y_trains, cv=5, scoring='accuracy').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6790564689063617\n",
      "Precision: 0.6886797470037577\n",
      "Recall:0.6790564689063617\n",
      "F1 Score: 0.6748658600777743\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_tests,result)\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "#precision\n",
    "precision = precision_score(y_tests,result,average='weighted')\n",
    "print(f'Precision: {precision}')\n",
    "#Recall\n",
    "recall = recall_score(y_tests, result, average='weighted')\n",
    "print(f'Recall:{recall}')\n",
    "#f1 Score \n",
    "f1 = f1_score(y_tests,result,average='weighted')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.21586847748391708\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy = 'most_frequent', random_state=42)\n",
    "\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'Baseline Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost in comprasion to the linear svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m\n\u001b[1;32m     23\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(\n\u001b[1;32m     24\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGB Predictions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     xaxis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Subreddit\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Show the figure\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Convert Plotly fig to a static image bytes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m img_bytes \u001b[38;5;241m=\u001b[39m pio\u001b[38;5;241m.\u001b[39mto_image(fig, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/699-capstone/.venv/lib/python3.9/site-packages/plotly/basedatatypes.py:3410\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/699-capstone/.venv/lib/python3.9/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "#XGB\n",
    "predicted_subreddit_names = label_encoder.inverse_transform(results)\n",
    "true_subreddit_names = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_subreddit_names, predicted_subreddit_names)\n",
    "\n",
    "# Get unique class names in the right order\n",
    "sorted_unique_names = np.unique(np.concatenate((true_subreddit_names, predicted_subreddit_names)))\n",
    "\n",
    "# Create the heatmap\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=conf_matrix,\n",
    "    x=sorted_unique_names.tolist(),  \n",
    "    y=sorted_unique_names.tolist(),  \n",
    "    colorscale='Viridis',\n",
    "    annotation_text=np.around(conf_matrix.astype(float) / conf_matrix.sum(axis=1)[:, np.newaxis], decimals=2).astype(str),\n",
    "    hoverinfo=\"z\",\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='XGB Predictions',\n",
    "    xaxis=dict(title='Predicted Subreddit'),\n",
    "    yaxis=dict(title='True Subreddit', autorange='reversed'),\n",
    "    width=800,  \n",
    "    height=800,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "# Convert Plotly fig to a static image bytes\n",
    "img_bytes = pio.to_image(fig, format='png')\n",
    "\n",
    "# Display the static image\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear svc was choose due to it simplicity fronm there we tune with two techiques the costly yet powerful grid search and Bayesian optimization. Bayesian learns statistically which combinations did better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear SVC\n",
    "predicted_subreddit_names = label_encoder.inverse_transform(result)\n",
    "true_subreddit_names = label_encoder.inverse_transform(y_tests)\n",
    "conf_matrix = confusion_matrix(y_tests, result)\n",
    "\n",
    "# Ensure it's in list format\n",
    "subreddit_names_list = sorted_unique_names.tolist()\n",
    "\n",
    "# Re-attempt to create the heatmap\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=conf_matrix,\n",
    "    x=subreddit_names_list,  # Ensure this is a list\n",
    "    y=subreddit_names_list,  # Ensure this is a list too\n",
    "    colorscale='Viridis',\n",
    "    annotation_text=np.around(conf_matrix.astype(np.float64) / conf_matrix.sum(axis=1)[:, np.newaxis], decimals=2),\n",
    "    hoverinfo=\"z\",\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Linear SVC Predictions',\n",
    "    xaxis=dict(title='Predicted Subreddit'),\n",
    "    yaxis=dict(title='True Subreddit'),\n",
    "    yaxis_autorange='reversed'\n",
    ")\n",
    "fig.show()\n",
    "# Convert Plotly fig to a static image bytes\n",
    "img_bytes = pio.to_image(fig, format='png')\n",
    "\n",
    "# Display the static image\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put predictions back into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = XGB.predict(combined_features)\n",
    "\n",
    "# Decode predictions to original labels\n",
    "decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Add predictions back to the DataFrame\n",
    "df['predicted_subreddit'] = decoded_predictions\n",
    "\n",
    "df.head(5)\n",
    "# df.to_csv('Final_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
