{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlieu/Library/Python/3.9/lib/python/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOCALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adadum</td>\n",
       "      <td>I wish I could do a poll for this but what wou...</td>\n",
       "      <td>2021-12-23T17:56:36.000Z</td>\n",
       "      <td>rn19yc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aspergers</td>\n",
       "      <td>Separatism for high-functioning Autists?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/aspergers/comments/rn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lazy_Opposite8263</td>\n",
       "      <td>I was diagnosed with high functioning asperger...</td>\n",
       "      <td>2021-12-23T17:53:41.000Z</td>\n",
       "      <td>rn17lw</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>aspergers</td>\n",
       "      <td>Friend blaming toxic behaviour on autism? How ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/aspergers/comments/rn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LockedOutOfElfland</td>\n",
       "      <td>It's been a while since I've seen a therapist ...</td>\n",
       "      <td>2021-12-23T17:28:07.000Z</td>\n",
       "      <td>rn0n6k</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>aspergers</td>\n",
       "      <td>Do therapists/mental health professionals misi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/aspergers/comments/rn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PlazmaSkittles</td>\n",
       "      <td>New here.  22 years old.  Diagnosed at 12.  AD...</td>\n",
       "      <td>2021-12-23T16:54:23.000Z</td>\n",
       "      <td>rmzvwn</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>aspergers</td>\n",
       "      <td>I think I'm broken</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/aspergers/comments/rm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xime_rollin</td>\n",
       "      <td>based on your understanding</td>\n",
       "      <td>2021-12-23T16:53:43.000Z</td>\n",
       "      <td>rmzvds</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aspergers</td>\n",
       "      <td>I recently took an eq test and scored 20 out o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/aspergers/comments/rm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body  \\\n",
       "0              Adadum  I wish I could do a poll for this but what wou...   \n",
       "1   Lazy_Opposite8263  I was diagnosed with high functioning asperger...   \n",
       "2  LockedOutOfElfland  It's been a while since I've seen a therapist ...   \n",
       "3      PlazmaSkittles  New here.  22 years old.  Diagnosed at 12.  AD...   \n",
       "4         xime_rollin                        based on your understanding   \n",
       "\n",
       "                created_utc      id  num_comments  score  subreddit  \\\n",
       "0  2021-12-23T17:56:36.000Z  rn19yc             0      1  aspergers   \n",
       "1  2021-12-23T17:53:41.000Z  rn17lw             1      1  aspergers   \n",
       "2  2021-12-23T17:28:07.000Z  rn0n6k             2      3  aspergers   \n",
       "3  2021-12-23T16:54:23.000Z  rmzvwn             3      1  aspergers   \n",
       "4  2021-12-23T16:53:43.000Z  rmzvds             0      1  aspergers   \n",
       "\n",
       "                                               title  upvote_ratio  \\\n",
       "0           Separatism for high-functioning Autists?           1.0   \n",
       "1  Friend blaming toxic behaviour on autism? How ...           1.0   \n",
       "2  Do therapists/mental health professionals misi...           1.0   \n",
       "3                                 I think I'm broken           1.0   \n",
       "4  I recently took an eq test and scored 20 out o...           1.0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/aspergers/comments/rn...  \n",
       "1  https://www.reddit.com/r/aspergers/comments/rn...  \n",
       "2  https://www.reddit.com/r/aspergers/comments/rn...  \n",
       "3  https://www.reddit.com/r/aspergers/comments/rm...  \n",
       "4  https://www.reddit.com/r/aspergers/comments/rm...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # condense into one df \n",
    "df1 = pd.read_csv('../Datasets/aspergers.csv', parse_dates=True)\n",
    "df2 = pd.read_csv('../Datasets/depression.csv', parse_dates=True)\n",
    "df3 = pd.read_csv('../Datasets/ocd.csv', parse_dates=True)\n",
    "df4 = pd.read_csv('../Datasets/ptsd.csv', parse_dates=True)\n",
    "df5 = pd.read_csv('../Datasets/adhd.csv', parse_dates=True)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4,df5], ignore_index=True)\n",
    "#LOWER ALL subreddit \n",
    "df['subreddit'] = df['subreddit'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['aspergers', 'depression', 'ocd', 'ptsd', 'adhd'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_subreddit_count = df['subreddit'].isnull().sum()\n",
    "print(null_subreddit_count)\n",
    "\n",
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exported sample size of post for external regex validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testy = df['body'].sample(n=50, random_state=1)\n",
    "df_testy_df = df_testy.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to hot encode text variables we want to include in the model. For example Body is a post that is text based will convert this in to a sentiment score \n",
    "Also subreddit will be our target variable so we need to preform mapping on this column and then excluded the columns we will not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text before performing sentiment analysis my hope is to improve optimization and accuracy improvement\n",
    "\n",
    "def clean_text(text: str, slang_map: dict = None, stopwords = None) -> str:\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions (@username)\n",
    "    text = re.sub(r'\\@\\w+', '', text)\n",
    "    # Remove hashtags (but keep the text)\n",
    "    text = re.sub(r'\\#\\w+', '', text)\n",
    "    # Remove HTML entities (e.g., &amp;, &lt;, etc.)\n",
    "    text = re.sub(r'\\&\\w+;', '', text)\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Optional: Convert slang or shorthand notations to full form\n",
    "    if slang_map:\n",
    "        for slang, full_form in slang_map.items():\n",
    "            text = text.replace(slang, full_form)\n",
    "    # Optional: Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Optional: Remove stopwords\n",
    "    if stopwords:\n",
    "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    # Optional: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.063263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  num_comments  score  upvote_ratio  sentiment_score  target\n",
       "0       1             0      1           1.0         0.412500       1\n",
       "1       1             1      1           1.0        -0.063263       1\n",
       "2       1             2      3           1.0         0.021373       1\n",
       "3       1             3      1           1.0         0.013254       1\n",
       "4       1             0      1           1.0         0.000000       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping \n",
    "subreddit_to_target = {\n",
    "    'aspergers': 1,\n",
    "    'depression': 2,\n",
    "    'ocd': 3,\n",
    "    'ptsd': 4,\n",
    "    'adhd' :5,\n",
    "}\n",
    "\n",
    "df['target'] = df['subreddit'].map(subreddit_to_target)\n",
    "\n",
    "# hot coding body to sentiment_score \n",
    "df['clean_body'] = df['body'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment_score'] = df['clean_body'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "#setting target varibale \n",
    "test_df = df[['target','num_comments','score','upvote_ratio','sentiment_score','target']]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents_train: 0         i wish i could do a poll for this but what wou...\n",
      "1         i was diagnosed with high functioning asperger...\n",
      "2         its been a while since ive seen a therapist mo...\n",
      "3         new here 22 years old diagnosed at 12 adhd as ...\n",
      "4                               based on your understanding\n",
      "                                ...                        \n",
      "151283                                              removed\n",
      "151284    since the dsm was recently updated add now fal...\n",
      "151285    i have a particularly worrying mental health i...\n",
      "151286    i have these meds that i have to take in the m...\n",
      "151287    i am supposed to be taking 18mg concerta but i...\n",
      "Name: body, Length: 151288, dtype: object\n",
      "feature_names: ['aaaaand' 'aaaand' 'aaron' ... 'zoophilia' 'zopiclone' 'zyprexa']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#mapping \n",
    "subreddit_to_target = {\n",
    "    'aspergers': 1,\n",
    "    'depression': 2,\n",
    "    'ocd': 3,\n",
    "    'ptsd': 4,\n",
    "    'adhd' :5,\n",
    "}\n",
    "\n",
    "df['target'] = df['subreddit'].map(subreddit_to_target)\n",
    "\n",
    "# hot coding body to sentiment_score \n",
    "documents_train = df['body'].apply(lambda x: clean_text(str(x)))\n",
    "print(f\"documents_train: {documents_train}\")\n",
    "\n",
    "# Apply topic modeling \n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,  # only top 5k by freq\n",
    "    lowercase=True,  # drop capitalization\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=2,  # note: absolute count of doc\n",
    "    max_df=0.05,  # note: % of docs\n",
    "    token_pattern=r\"\\b[a-z]{3,12}\\b\",  # remove short, non-word-like terms\n",
    "    stop_words=\"english\",\n",
    ")  # default English stopwords\n",
    "\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"feature_names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pickle\n",
    "\n",
    "f = open(\"./text8_W2V.pickle\", \"rb\")\n",
    "text8_model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def topical_coherence(terms):\n",
    "    embeddings = []\n",
    "    for term in terms:\n",
    "        try:\n",
    "            embeddings.append(text8_model.transform(term)[0])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if len(embeddings) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_sim = cosine_similarity(embeddings)\n",
    "        np.fill_diagonal(cos_sim, 0)\n",
    "        return cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17541161,\n",
       " 0.22177409,\n",
       " 0.21064776,\n",
       " 0.22773312,\n",
       " 0.26023465,\n",
       " 0.27220905,\n",
       " 0.2631486,\n",
       " 0.24826024,\n",
       " 0.24761842]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "result = []\n",
    "for n in range(2, 11):\n",
    "    nmf = NMF(n_components=n, random_state=42, init=\"nndsvd\")\n",
    "    W = nmf.fit_transform(tfidf_documents)\n",
    "    H = nmf.components_\n",
    "    \n",
    "    coherences = []\n",
    "    for topic_index in range(0, n):\n",
    "        top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "        top_terms = []\n",
    "        for term_index in top_indices[0:10]:\n",
    "            top_terms.append(feature_names[term_index])\n",
    "        coherences.append(topical_coherence(top_terms))\n",
    "\n",
    "    result.append(np.median(coherences))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0 ['nan', 'passes', 'alive', 'probably', 'world', 'mum', 'inspire', 'dad']\n",
      "topic  1 ['job', 'understand', 'friend', 'parents', 'wanted', 'home', 'stuff', 'mom']\n",
      "topic  2 ['medication', 'meds', 'taking', 'adderall', 'doctor', 'vyvanse', 'dose', 'effects']\n",
      "topic  3 ['sleep', 'night', 'tired', 'wake', 'bed', 'hours', 'asleep', 'sleeping']\n",
      "topic  4 ['intrusive', 'compulsions', 'scared', 'head', 'real', 'fear', 'compulsion', 'happen']\n",
      "topic  5 ['ptsd', 'trauma', 'therapy', 'therapist', 'happened', 'symptoms', 'panic', 'flashbacks']\n",
      "topic  6 ['hate', 'fucking', 'shit', 'tired', 'fuck', 'wish', 'die', 'wanna']\n"
     ]
    }
   ],
   "source": [
    "n_topics = 7\n",
    "X = tfidf_documents\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=0, init=\"nndsvd\")\n",
    "W = nmf.fit_transform(X) \n",
    "H = nmf.components_\n",
    "\n",
    "top = 8\n",
    "topic_index_max = n_topics\n",
    "\n",
    "for topic_index in range(0, topic_index_max):\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(feature_names[term_index])\n",
    "    print(\"topic \", topic_index, top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151288, 101960)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_documents = tf_vectorizer.fit_transform(documents_train)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(tf_documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 7\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# This could take a few minutes to run...\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, random_state=0)\n",
    "lda.fit(tf_documents)\n",
    "topic_models = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0: ['removed', 'im', 'job', 'school', 'work', 'just', 'year', 'college']\n",
      "topic 1: ['im', 'adhd', 'ive', 'medication', 'diagnosed', 'just', 'meds', 'taking']\n",
      "topic 2: ['im', 'like', 'ocd', 'just', 'feel', 'thoughts', 'dont', 'know']\n",
      "topic 3: ['people', 'like', 'just', 'im', 'dont', 'things', 'time', 'think']\n",
      "topic 4: ['im', 'just', 'like', 'feel', 'dont', 'ive', 'time', 'really']\n",
      "topic 5: ['deleted', 'im', 'just', 'didnt', 'like', 'told', 'said', 'know']\n",
      "topic 6: ['just', 'im', 'dont', 'life', 'want', 'like', 'feel', 'know']\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 8\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        term_list = [feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        print(\"topic %d:\" % (topic_idx), term_list)\n",
    "\n",
    "\n",
    "display_topics(lda, tf_feature_names, num_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
