{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from pmdarima.arima import auto_arima\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tqdm import tqdm\n",
    "#import itertools\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tpot import TPOTClassifier\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot coding body to sentiment_score \n",
    "df['sentiment_score'] = df['body'].apply(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "#setting target varibale \n",
    "df[['target','num_comments','score','upvote_ratio','sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114179, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# condense into one df \n",
    "df1 = pd.read_csv('C:/Users/dross/Desktop/Project/aspergers.csv', parse_dates=True)\n",
    "df2 = pd.read_csv('C:/Users/dross/Desktop/Project/depression.csv', parse_dates=True)\n",
    "df3 = pd.read_csv('C:/Users/dross/Desktop/Project/ocd.csv', parse_dates=True)\n",
    "df4 = pd.read_csv('C:/Users/dross/Desktop/Project/ptsd.csv', parse_dates=True)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to hot encode text variables we want to include in the model. For example Body is a post that is text based will convert this in to a sentiment score \n",
    "Also subreddit will be our target variable so we need to preform mapping on this column and then excluded the columns we will not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text before performing sentiment analysis my hope is to improve optimization and accuracy improvement\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114179, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping \n",
    "subreddit_to_target = {\n",
    "    'aspergers': 1,\n",
    "    'depression': 2,\n",
    "    'OCD': 3,\n",
    "    'ptsd': 4\n",
    "}\n",
    "\n",
    "df['target'] = df['subreddit'].map(subreddit_to_target)\n",
    "\n",
    "# hot coding body to sentiment_score \n",
    "df['clean_body'] = df['body'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment_score'] = df['clean_body'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "#setting target varibale \n",
    "test_df = df[['target','num_comments','score','upvote_ratio','sentiment_score','target']]\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
